{"cells":[{"metadata":{"_uuid":"4b6246b4-c33b-4bc7-baf3-de6e9b74c5db","_cell_guid":"fd975313-4f0b-4644-b5da-d40a7310deaf","trusted":true},"cell_type":"code","source":"# new logs\n# finetune start = 159 epoch\n# use_mean beta at finetune","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"911ef5fa-0d6f-4206-a579-536fcdd8ef67","_cell_guid":"04577c6c-832e-4d88-bf2c-5edf57e3d6ae","trusted":true},"cell_type":"code","source":"!pip install faiss-gpu==1.6.1\n!pip install tensorboardX\n!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n!pip install mlflow","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"63e45aa9-b997-4e85-aa0b-eabba0fb9677","_cell_guid":"6a693f12-3d35-4f99-8ea6-02c48ac50a93","trusted":true},"cell_type":"code","source":"import torch\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport torch.nn as nn\nfrom torch.nn import init\n# from apex import amp\nimport torch.nn.functional as f\nimport torchvision\nfrom math import ceil\nfrom PIL import Image\nimport os\nfrom torchvision import transforms\nfrom scipy.optimize import linear_sum_assignment\nfrom sklearn.metrics.cluster import normalized_mutual_info_score as nmi_\nfrom scipy.optimize import linear_sum_assignment as lsa\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\nimport tarfile\nimport scipy.io\nimport faiss\nimport warnings\nfrom warmup_scheduler import GradualWarmupScheduler\nimport gc\nfrom scipy.special import softmax\nimport mlflow\nfrom collections import Counter\nfrom shutil import copyfile\nwarnings.filterwarnings('ignore')\ntorch.cuda.empty_cache()\n\n# params.py\n\nsop_dataset_path = '/kaggle/input/sopdataset/Stanford_Online_Products/'\ninshop_dataset_path = '/kaggle/input/inshopclothes/In_shop Clothes Retrieval Benchmark'\ncub_dataset_path = '/kaggle/input/cub200/CUB_200_2011'\ncars_dataset_path = '/kaggle/input/cars196'\npretrained_pytorch_models = '/kaggle/input/pretrained-pytorch-models'\ninput_dir = '/kaggle/input/checkpoints'\n\ncheckpoints_dir = '/kaggle/working/'\ntensorboard_dir = '/kaggle/working/'\nlosses_dir = '/kaggle/working/'\nmetrics_dir = '/kaggle/working/'\ndensity_plots_dir = '/kaggle/working/'\n\ndevice = 'cuda:0'\nrandom_seed = 0\nlr = 5e-5\nembedding_dim = 128\nbatch_size = 80\nmasks_lr = 5e-5\n\n# OnlineProducts params\nsop_train_classes = range(0, 11318)\nsop_test_classes = range(11318, 22634)\nsop_nb_train_data = 59551\n\n# InShop params\ninshop_train_classes = range(0, 3997)\ninshop_test_classes = range(0, 3985)\ninshop_nb_train_data = 25882\n\n# CARS196 params\ncars_train_classes = range(1, 99)\ncars_test_classes = range(99, 197)\ncars_nb_train_data = 8054\n\n# CUB200 params\ncub_train_classes = range(1, 101)\ncub_test_classes = range(101, 201)\ncub_nb_train_data = 5864\n\nepochs = 250\nfine_tune_epoch = 250 # 190\nrecluster_epoch_freq = 2\n\nload_saved_model_for_test = True\nsave_clusters = True\nsave_model = True\nto_make_density_plots = True\n\ncontinue_training_from_epoch = True\ncheckpoint_from_epoch = 211\nclusters_from_epoch = 210\nload_all_metrics = True  # True if continue training\nall_metrics_dir = f'/kaggle/input/checkpoints/all_metrics_{checkpoint_from_epoch}'\n\n# to eval model with loaded checkpoint or to check model performance before training\ncompute_metrics_before_training = False\nevalaute_on_train_data = False\nlambda1 = 5e-4  # masks l1 regularization\nlambda2 = 5e-3\n\navailable_datasets = [\n    'OnlineProducts',\n    'InShopClothes',\n    'CUB200',\n    'CARS196'\n]\ncur_dataset = 'OnlineProducts'\nuse_gem = True\nuse_lr_scheduler = True\nuse_mean_beta_at_finetune = True  # False\nuse_cls_loss = False\n\nif cur_dataset is 'CUB200' or cur_dataset is 'CARS196':\n    nb_clusters = 4\nelse:\n    nb_clusters = 8\n    \n# ft_epoch = 60 if cur_dataset is 'CUB200' or cur_dataset is 'CARS196' else 80\nft_epoch = 159  # 100\n\n# MLFlow params\nworking_dir = '/kaggle/working/'\nparams_save = {key: value for key, value in locals().items() if 'gc' not in key and not key.startswith('__')}\nmlflow.set_experiment('SOP masks baseline relu')\nfor key, value in params_save.items():\n    mlflow.log_param(key, value)\n\n    # artifact_path = mlflow.get_artifact_uri()\n# mlflow.log_artifact(os.path.abspath(__file__))\ndirs_for_logs = ['distances', 'masks', 'loss', 'clusters', 'eval', 'artifacts']\nfor dir_ in dirs_for_logs:\n    if dir_ not in os.listdir(metrics_dir):\n        os.makedirs(os.path.join(metrics_dir, dir_))\n\n# loading all previous metrics logs in Kaggle\nif load_all_metrics:\n    dirs_to_make = os.listdir(all_metrics_dir)\n    for dir_ in dirs_to_make:\n        if dir_ not in os.listdir(metrics_dir):\n            os.makedirs(os.path.join(metrics_dir, dir_))\n    for dir_ in dirs_to_make:\n        metrics_to_load = os.listdir(os.path.join(all_metrics_dir, dir_))\n        for metr in metrics_to_load:\n            copyfile(os.path.join(all_metrics_dir, os.path.join(dir_, metr)), os.path.join(metrics_dir, os.path.join(dir_, metr)))\n    print(f'loaded previous metrics logs')\n\ndef log_metrics(path, metrics, step, with_mlflow=True):\n    time_step = str(time.time()).replace('.', '')[:13]\n    with open(os.path.join(metrics_dir, path), 'a') as f:\n        f.write(f'{time_step} {metrics} {step}\\n')\n    if with_mlflow:\n        mlflow.log_metric(path, metrics, step)\n\ndef log_artifacts(artifact_path, file_name, with_mlflow=True):\n    copyfile(artifact_path, os.path.join(os.path.join(metrics_dir, 'artifacts'), file_name))\n    if with_mlflow:\n        mlflow.log_artifact(artifact_path)    \n\n\nfrom math import ceil\nimport numpy as np\nimport torchvision\nimport torch\nfrom torch.nn import Linear, Dropout, AvgPool2d, MaxPool2d\nfrom torch.nn.init import xavier_normal_\n\ndef gem(x, p=3, eps=1e-6):\n    return torch.flatten(f.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p), 1)\n    # return F.lp_pool2d(F.threshold(x, eps, eps), p, (x.size(-2), x.size(-1))) # alternative\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = torch.nn.parameter.Parameter(torch.ones(1) * p, requires_grad=True)\n        self.eps = eps\n\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(\n            self.eps) + ')'\n\ndef get_resnet50():\n    resnet50 = torchvision.models.resnet50(pretrained=True)\n    resnet50.features = torch.nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool, resnet50.layer1,\n                                            resnet50.layer2, resnet50.layer3, resnet50.layer4)\n    resnet50.sz_features_output = 2048\n    for module in filter(lambda m: type(m) == torch.nn.BatchNorm2d, resnet50.modules()):\n        module.eval()\n        module.train = lambda _: None\n    return resnet50\n\n\ndef get_params_dict(model, emb_module_name=['embedding', 'masks']):\n    dict_ = {k: [] for k in ['backbone', *emb_module_name]}\n    for name, param in model.named_parameters():\n        name = name.split('.')[0]\n        if name not in emb_module_name:\n            dict_['backbone'] += [param]\n        else:\n            dict_[name] += [param]\n    nb_total = len(list(model.parameters()))\n    nb_dict_params = sum([len(dict_[d]) for d in dict_])\n    assert nb_total == nb_dict_params\n    return dict_\n\n\ndef get_embedding(model):\n    if use_gem:\n        model.features_pooling = GeM().to(device)\n    else:\n        model.features_pooling = AvgPool2d(7, stride=1, padding=0, ceil_mode=True, count_include_pad=True)\n    model.features_dropout = Dropout(0.01)\n    torch.random.manual_seed(1)\n    model.embedding = Linear(model.sz_features_output, embedding_dim).to(list(model.parameters())[0].device)\n    model.classification_layer = nn.Linear(model.sz_features_output, len(sop_train_classes)).to(device)\n    model.masks = torch.nn.Embedding(nb_clusters, embedding_dim)\n    # initialize weights\n    model.masks.weight.data.normal_(0.9, 0.7)  # 0.1, 0.005\n    torch.random.manual_seed(1)\n    np.random.seed(1)\n    # _xavier_init\n    model.embedding.weight.data = xavier_normal_(model.embedding.weight.data, gain=1)\n    features_parameters = model.features.parameters()\n    model.parameters_dict = get_params_dict(model=model)\n\n    def forward(x, with_embedding_layer=True, learner_id=None, with_softmax=False):\n        x = model.features(x)\n        x = model.features_pooling(x)\n        x = model.features_dropout(x)\n        features = x.view(x.size(0), -1)\n        if with_embedding_layer:\n            x = model.embedding(features)\n            if with_softmax:\n                return x, model.classification_layer(features)\n            return x\n        else:\n            embeddings = model.embedding(features)\n            return features, embeddings\n    model.forward = forward\n\n\ndef get_model():\n    resnet50 = get_resnet50()\n    get_embedding(resnet50)\n    return resnet50\n\n#  OnlineProducts Dataset\n\nclass OnlineProductsDataset(torch.utils.data.Dataset):\n    def __init__(self, ds_path, mode, transforms_mode='test', train_st_class=0, train_fin_class=11318, test_st_class=11318, test_fin_class=22634,\n                 nb_train_imgs=59551, nb_test_imgs=60502):\n        self.mode = mode\n        self.transforms_mode = transforms_mode\n        self.sz_crop = 224\n        self.sz_resize = 256\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n        if self.mode == 'train':\n            self.classes = range(train_st_class, train_fin_class)\n        else:\n            self.classes = range(test_st_class, test_fin_class)\n        self.ds_path = ds_path\n        self.labels, self.img_paths, self.idxs = list(), list(), list()\n\n        im_paths_list = os.path.join(ds_path, f'Ebay_{self.mode}.txt')\n        with open(im_paths_list) as f:\n            f.readline()\n            idx, nb_imgs = 0, 0\n\n            for (image_id, class_id, _, path) in map(str.split, f):\n                nb_imgs += 1\n                if int(class_id) - 1 in self.classes:\n                    self.img_paths.append(os.path.join(self.ds_path, path))\n                    self.labels.append(int(class_id) - 1)\n                    self.idxs.append(idx)\n                    idx += 1\n\n        if self.mode == 'train':\n            assert nb_imgs == nb_train_imgs\n        else:\n            assert nb_imgs == nb_test_imgs\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        im = Image.open(self.img_paths[index])\n        if len(list(im.split())) == 1:\n            im = im.convert('RGB')\n        im = self.apply_augmentation(im)\n        return im, self.labels[index], index\n\n    def apply_augmentation(self, im):\n        if self.transforms_mode == 'train':\n            transforms_ = transforms.Compose([\n                transforms.RandomResizedCrop(self.sz_crop),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        else:\n            transforms_ = transforms.Compose([\n                transforms.Resize(self.sz_resize),\n                transforms.CenterCrop(self.sz_crop),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        return transforms_(im)\n\n    def get_nb_classes(self):\n        return len(self.classes)\n\n    def get_label(self, index):\n        return self.labels[index]\n\n    def get_within_indexes(self, indexes):\n        self.labels = [self.labels[i] for i in indexes]\n        self.idxs = [self.idxs[i] for i in indexes]   ###\n        self.img_paths = [self.img_paths[i] for i in indexes]\n\n\n# InShopClothes Dataset\n\nclass InShopDataset(torch.utils.data.Dataset):\n    def __init__(self, ds_path, transforms_mode='test', ds_info_path='Eval/list_eval_partition.txt', mode='train', nb_train_images=25882,\n                 nb_query_images=14218, nb_gallery_images=12612, nb_train_classes=3997, nb_query_classes=3985,\n                 nb_gallery_classes=3985):\n\n        ''' nb_all_images = 52712 '''\n        self.ds_info_path = os.path.join(ds_path, ds_info_path)\n        self.mode = mode\n        self.transforms_mode = transforms_mode\n        self.paths, self.labels, self.ids = [], [], []\n\n        self.sz_crop = 224\n        self.sz_resize = 256\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n        with open(self.ds_info_path, 'r') as f:\n            ds_info = f.readlines()[2:]\n\n        for i, inf_ in enumerate(ds_info):\n            inf_ = inf_.replace('\\n', '').split()\n            path, lbl, mode = inf_\n            for el in [path, lbl, mode]:\n                assert el != ''\n            if mode == self.mode:\n                self.paths.append(os.path.join(ds_path, path))\n                self.labels.append(int(lbl[3:]))\n                self.ids.append(i)\n\n        ordered_labels = {lbl: i for i, lbl in enumerate(sorted(set(self.labels)))}\n        self.labels = list(map(lambda l: ordered_labels[l], self.labels))\n        if self.mode is 'train':\n            nb_images, nb_classes = nb_train_images, nb_train_classes\n        elif self.mode is 'query':\n            nb_images, nb_classes = nb_query_images, nb_query_classes\n        else:\n            nb_images, nb_classes = nb_gallery_images, nb_gallery_classes\n        assert len(self.labels) == nb_images\n        assert len(set(self.labels)) == nb_classes\n\n    def apply_augmentation(self, img):\n        if self.transforms_mode == 'train':\n            transforms_ = transforms.Compose([\n                transforms.RandomResizedCrop(self.sz_crop),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        else:\n            transforms_ = transforms.Compose([\n                transforms.Resize(self.sz_resize),\n                transforms.CenterCrop(self.sz_crop),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        return transforms_(img)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_nb_classes(self):\n        return len(set(self.labels))\n\n    def unique_labels(self):\n        return set(self.labels)\n\n    def __getitem__(self, idx):\n        self.paths[idx] = self.paths[idx].replace('/img/', '/Img/')\n        assert os.path.exists(self.paths[idx])\n        try:\n            # assert os.path.exists(self.paths[idx])\n            img = Image.open(self.paths[idx])\n        except:\n            print(self.paths[idx])\n\n        if len(list(img.split())) != 3:\n            img = img.convert('RGB')\n        img = self.apply_augmentation(img)\n        return img, self.labels[idx], idx\n\n    def get_label(self, idx):\n        return self.labels[idx]\n\n    def get_within_indexes(self, idxs):\n        self.paths = [self.paths[i] for i in idxs]\n        self.labels = [self.labels[i] for i in idxs]\n        self.ids = [self.ids[i] for i in idxs]\n\n# CUB200 Dataset\n\nclass CUBDataset(torch.utils.data.Dataset):\n    def __init__(self, ds_path, mode, transforms_mode='test', nb_train_images=5864, nb_test_images=5924):\n\n        self.ds_path = ds_path\n        self.mode = mode\n        self.transforms_mode = transforms_mode\n        self.paths, self.labels, self.ids = [], [], []\n\n        self.sz_crop = 224\n        self.sz_resize = 256\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n        with open(os.path.join(self.ds_path, 'images.txt'), 'r') as f:\n            images = f.readlines()\n\n        with open(os.path.join(self.ds_path, 'image_class_labels.txt'), 'r') as f:\n            image_class_labels = f.readlines()\n\n        for labels, paths in zip(image_class_labels, images):\n            id1, label = labels.replace('\\n', '').split()\n            id2, path = paths.replace('\\n', '').split()\n            assert int(id1) == int(id2)\n\n            # using first 100 classes for training\n            if mode is 'train' and int(label) < 101:\n                self.labels.append(int(label))\n                self.paths.append(path)\n                self.ids.append(int(id1) - 1)\n\n            # and second 100 classes for testing\n            elif mode is 'test' and int(label) >= 101:\n                self.labels.append(int(label))\n                self.paths.append(path)\n                self.ids.append(int(id1) - 1)\n\n        if mode is 'train':\n            assert len(self.labels) == nb_train_images\n        else:\n            assert len(self.labels) == nb_test_images\n\n    def apply_augmentation(self, img):\n        if self.transforms_mode == 'train':\n            transforms_ = transforms.Compose([\n                transforms.RandomResizedCrop(self.sz_crop),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        else:\n            transforms_ = transforms.Compose([\n                transforms.Resize(self.sz_resize),\n                transforms.CenterCrop(self.sz_crop),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        return transforms_(img)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_nb_classes(self):\n        return len(set(self.labels))\n\n    def unique_labels(self):\n        return set(self.labels)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.ds_path + '/images/' + self.paths[idx])\n        if len(list(img.split())) != 3:\n            img = img.convert('RGB')\n        img = self.apply_augmentation(img)\n        return img, self.labels[idx], idx\n\n    def get_label(self, idx):\n        return self.labels[idx]\n\n    def get_within_indexes(self, idxs):\n        self.paths = [self.paths[i] for i in idxs]\n        self.labels = [self.labels[i] for i in idxs]\n        self.ids = [self.ids[i] for i in idxs]\n\n# CARS196 Dataset\n\nclass CARSDataset(torch.utils.data.Dataset):\n    def __init__(self, ds_path, mode, transforms_mode='test', nb_train_images=8054, nb_test_images=8131, nb_all=16185):\n\n        self.ds_path = ds_path\n        self.mode = mode\n        self.transforms_mode = transforms_mode\n        self.paths, self.labels, self.ids = [], [], []\n\n        self.sz_crop = 224\n        self.sz_resize = 256\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n        mat = scipy.io.loadmat(os.path.join(ds_path, 'cars_annos.mat'))\n        id = 0\n        for annotation in mat['annotations'][0]:\n            label = int(str(np.squeeze(annotation['class'])))\n            if mode is 'train' and label < 99:\n                self.paths.append(str(np.squeeze(annotation['relative_im_path'])))\n                self.labels.append(int(str(np.squeeze(annotation['class']))))\n                self.ids.append(id)\n                id += 1\n            elif mode is 'test' and label >= 99:\n                self.paths.append(str(np.squeeze(annotation['relative_im_path'])))\n                self.labels.append(int(str(np.squeeze(annotation['class']))))\n                self.ids.append(id)\n                id += 1\n\n        if mode is 'train':\n            assert len(self.labels) == nb_train_images\n        else:\n            assert len(self.labels) == nb_test_images\n\n    def apply_augmentation(self, img):\n        if self.transforms_mode == 'train':\n            transforms_ = transforms.Compose([\n                transforms.RandomResizedCrop(self.sz_crop),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        else:\n            transforms_ = transforms.Compose([\n                transforms.Resize(self.sz_resize),\n                transforms.CenterCrop(self.sz_crop),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=self.mean,\n                    std=self.std,\n                )\n            ])\n        return transforms_(img)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_nb_classes(self):\n        return len(set(self.labels))\n\n    def unique_labels(self):\n        return set(self.labels)\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.ds_path, self.paths[idx]))\n        if len(list(img.split())) != 3:\n            img = img.convert('RGB')\n        img = self.apply_augmentation(img)\n        return img, self.labels[idx], idx\n\n    def get_label(self, idx):\n        return self.labels[idx]\n\n    def get_within_indexes(self, idxs):\n        self.paths = [self.paths[i] for i in idxs]\n        self.labels = [self.labels[i] for i in idxs]\n        self.ids = [self.ids[i] for i in idxs]\n\nclass BatchSampler(torch.utils.data.sampler.BatchSampler):\n    def __init__(self, dataset, batch_size=batch_size, m=4):\n        self.m = m\n        self.dataset = dataset\n        self.batch_size = batch_size\n\n        self.prod_labels = np.array(dataset.labels)\n        self.unique_prod_labels = list(set(self.prod_labels))\n\n        self.prod_label_indices_within_cur_label = self.get_all_prod_labels_indices_with_cur_label()\n        self.nb_classes_to_pick = batch_size // m\n\n        self.indices_real_from_class = self.get_all_prod_labels_indices_with_cur_label()\n        self.indices_taken_from_class = np.zeros(len(self.unique_prod_labels))\n\n    def get_all_prod_labels_indices_with_cur_label(self):\n        out = []\n        for c in self.unique_prod_labels:\n            prod_label_indices = np.where(self.prod_labels == c)[0]\n            np.random.shuffle(prod_label_indices)\n            out.append(prod_label_indices)\n        return out\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size\n\n    def __iter__(self):\n        for _ in range(len(self.dataset) // self.batch_size):\n            if len(self.unique_prod_labels) > self.nb_classes_to_pick:\n                replace = False\n            else:\n                replace = True\n            chosen_classes = np.random.choice(\n                len(self.unique_prod_labels), self.nb_classes_to_pick,\n                replace=replace)\n            out = []\n            for cl in chosen_classes:\n                if self.indices_taken_from_class[cl] + self.m \\\n                        < len(self.indices_real_from_class[cl]):\n                    st = int(self.indices_taken_from_class[cl])\n                    chosen_indices = self.indices_real_from_class[cl][st:st + self.m]\n                else:\n                    chosen_indices = np.random.choice(self.indices_real_from_class[cl],\n                                                      self.m,\n                                                      replace=len(self.indices_real_from_class[cl]) < self.m)\n                out.extend(chosen_indices)\n                self.indices_taken_from_class[cl] += self.m\n\n                if self.indices_taken_from_class[cl] + self.m \\\n                        > len(self.indices_real_from_class[cl]):\n                    np.random.shuffle(self.indices_real_from_class[cl])\n                    self.indices_taken_from_class[cl] = 0\n            yield out\n\n\n# loss.py\n\nclass MarginLoss(torch.nn.Module):\n    def __init__(self, beta=1.2, margin=0.2, nu=0.0, dist_weighted_sampler_thr=0.5):\n        super(MarginLoss, self).__init__()\n        self.beta = torch.nn.Parameter(torch.tensor([beta]), requires_grad=True)\n        self.margin = margin\n        self.nu = nu\n        self.thr = dist_weighted_sampler_thr\n\n    @staticmethod\n    def get_distance(x):\n        mm = torch.mm(x, x.t())\n        dist = mm.diag().view((mm.diag().size()[0], 1))\n        dist = dist.expand_as(mm)\n        dist_ = dist + dist.t()\n        dist_ = (dist_ - 2 * mm).clamp(min=0)\n        return dist_.clamp(min=1e-4).sqrt()\n\n    def sample_triplets(self, embeddings, prod_labels, iter_=None, learner_id=None, for_cluster=False):\n        anchor_ids, pos_ids, neg_ids = [], [], []\n        if not torch.is_tensor(prod_labels):\n            prod_labels = torch.tensor(prod_labels)\n\n        distance = self.get_distance(embeddings)\n        p0 = prod_labels.clone().view(1, prod_labels.size()[0]).expand_as(distance)\n        p1 = prod_labels.view(prod_labels.size()[0], 1).expand_as(distance)\n        positives_ids = torch.eq(p0, p1).to(device, dtype=torch.float32) - (torch.eye(len(distance))).to(device)\n        \n        n_ids = ((positives_ids > 0) + (distance < self.thr)).to(device, dtype=torch.float32)\n        unused_neg_part = 0\n        if not for_cluster:\n            n_ids_all = (distance[positives_ids == 0]).to(device, dtype=torch.float32)\n            unused_neg_part = (torch.sum(n_ids)/torch.sum(n_ids_all)).item()\n            log_metrics('clusters/unused_neg_part_within_cutoff', unused_neg_part, iter_)\n            log_metrics(f'clusters/unused_neg_part_within_cutoff_learner_id_{learner_id}', unused_neg_part, iter_)   \n                    \n        total_neg_dist = torch.mean(distance[positives_ids == 0])\n        negatives_ids = n_ids * 1e6 + distance\n        to_retrieve_ids = max(1, min(int(positives_ids.data.sum()) // len(positives_ids), negatives_ids.size(1)))\n        negatives = negatives_ids.topk(to_retrieve_ids, dim=1, largest=False)[1]\n        negatives_ids_ = torch.zeros_like(negatives_ids.data).scatter(1, negatives, 1.0)\n\n        for i in range(len(distance)):\n            anchor_ids.extend([i] * (int(positives_ids.data.sum()) // len(positives_ids)))\n            pos_ids_ = np.atleast_1d(positives_ids[i].nonzero().squeeze().cpu().numpy())\n            neg_ids_ = np.atleast_1d(negatives_ids_[i].nonzero().squeeze().cpu().numpy())\n            pos_ids.extend(pos_ids_)\n            neg_ids.extend(neg_ids_)\n\n            if len(anchor_ids) != len(pos_ids) or len(anchor_ids) != len(neg_ids):\n                t = min(map(len, [anchor_ids, pos_ids, neg_ids]))\n                anchor_ids = anchor_ids[:t]\n                pos_ids = pos_ids[:t]\n                neg_ids = neg_ids[:t]\n        anchors, positives, negatives = embeddings[anchor_ids], embeddings[pos_ids], embeddings[neg_ids]\n        return anchor_ids, anchors, positives, negatives, total_neg_dist, unused_neg_part\n\n    def forward(self, embeddings, product_labels, iter_, learner_id=-1, for_cluster=False):\n        a_indices, anchors, positives, negatives, total_neg_dist, unused_neg_part = self.sample_triplets(embeddings, product_labels, iter_=iter_, learner_id=learner_id, for_cluster=for_cluster)\n        log_metrics('loss/beta', self.beta.data.tolist()[0], iter_)\n        log_metrics(f'loss/beta_{learner_id}', self.beta.data.tolist()[0], iter_)\n        \n        beta_reg_loss = torch.norm(self.beta, p=1) * self.nu if a_indices is not None else 0.0\n        d_ap = torch.sqrt(torch.sum((positives - anchors) ** 2, dim=1) + 1e-8)\n        d_an = torch.sqrt(torch.sum((negatives - anchors) ** 2, dim=1) + 1e-8)\n        d_pn = torch.sqrt(torch.sum((positives - negatives) ** 2, dim=1) + 1e-8)\n        pos_loss = torch.clamp(d_ap - self.beta[0] + self.margin, min=0.0)\n        neg_loss = torch.clamp(self.beta[0] - d_an + self.margin, min=0.0)\n        loss = (torch.sum(pos_loss + neg_loss) + beta_reg_loss) / int(torch.sum((pos_loss > 0.0) + (neg_loss > 0.0)))\n        log_metrics('loss/margin_loss', loss.item(), iter_)\n        if not for_cluster:\n            return loss, d_ap, d_an, d_pn, torch.sum(pos_loss > 0.0)/float(pos_loss.view(-1).size(0)), torch.sum(neg_loss > 0.0)/float(neg_loss.view(-1).size(0)), \\\n               float(neg_loss.view(-1).size(0)), total_neg_dist, int(torch.sum((pos_loss > 0.0) + (neg_loss > 0.0))), self.beta.data.tolist()[0], unused_neg_part\n        else:\n            return loss\n\n\ndef make_density_plots(epoch, pos_distances, neg_distances, path_to_save=density_plots_dir, save_fig=True,\n                       show=False, mode='valid', log_distances=False):\n    sns.set(color_codes=True)\n    sns.kdeplot(pos_distances, shade=True, color='r', label='anchor-positive')\n    sns.kdeplot(neg_distances, shade=True, color='b', label='anchor-negative')\n    plt.xlabel('Distance')\n    plt.ylabel('Density')\n    if save_fig:\n        plt.savefig(os.path.join(path_to_save, mode + f'_density_plot_at_epoch_{epoch}'))\n        if log_distances:\n            json.dump(list(pos_distances),\n                      open(os.path.join(path_to_save, 'a_p_dists_' + mode + f'_epoch_{epoch}'), 'w'))\n            json.dump(list(neg_distances),\n                      open(os.path.join(path_to_save, 'a_n_dists_' + mode + f'_epoch_{epoch}'), 'w'))\n        log_artifacts(os.path.join(path_to_save, mode + f'_density_plot_at_epoch_{epoch}.png'), mode + f'_density_plot_at_epoch_{epoch}.png')\n    if show:\n        plt.show()\n    plt.clf()\n\n\ndef get_criterion():\n    _criterion = [MarginLoss().cuda() for _ in range(nb_clusters+1)]\n    return _criterion\n\n\ndef get_optimizer(_model_, beta):\n    opt_ = torch.optim.Adam([\n        {'params': _model_.parameters_dict['backbone'], 'lr': lr},  # 'weight_decay': weight_decay},\n        {'params': _model_.parameters_dict['embedding'], 'lr': lr},  # 'weight_decay': weight_decay},\n        {'params':  _model_.parameters_dict['masks'], 'lr': masks_lr},\n        {'params': beta, 'lr': 0.01, 'weight_decay': 0.0001}])\n    return opt_\n\n\ndef kmeans(x, num_clusters, previous_centroids=None, nrns_clustering=False):\n    x = np.asarray(x.reshape(x.shape[0], -1), dtype=np.float32)\n    if nrns_clustering:\n        x = np.ascontiguousarray(x)\n    dim_ = x.shape[1]\n    kmeans = faiss.Clustering(x.shape[1], num_clusters)\n    # initializing clusters with previous centroids\n    if previous_centroids is not None:\n        print(f'clustering with previous centroids..')\n        kmeans.centroids.resize(previous_centroids.size)\n        faiss.memcpy(kmeans.centroids.data(), faiss.swig_ptr(previous_centroids), previous_centroids.size * 4)\n    kmeans.max_points_per_centroid = 10000000\n    kmeans.niter = 100\n    if previous_centroids is None:\n        kmeans.nredo = 5\n    else:\n        kmeans.nredo = 1\n    resources_ = faiss.StandardGpuResources()\n    idx_config_ = faiss.GpuIndexFlatConfig()\n    idx_config_.useFloat16 = False\n    idx_config_.device = 0\n    index = faiss.GpuIndexFlatL2(resources_, dim_, idx_config_)\n    if nrns_clustering:\n        x = np.ascontiguousarray(x)\n    kmeans.train(x, index)\n    centroids = faiss.vector_float_to_array(kmeans.centroids)\n    objective = faiss.vector_float_to_array(kmeans.obj)\n    centroids = centroids.reshape(num_clusters, dim_)\n    index = faiss.IndexFlatL2(centroids.shape[1])\n    index.add(centroids)\n    distances, labels = index.search(x, 1)\n    index.reset()\n    resources_.noTempMemory()\n    del kmeans\n    del index\n    del x\n    gc.collect()\n    return labels.ravel(), centroids\n\n\ndef get_nearest_neighbors(x, product_labels, k, queries=None):\n    x = np.asarray(x.reshape(x.shape[0], -1), dtype=np.float32)\n    with_queries = False\n    if queries is None:\n        with_queries = True\n        queries = x\n        k += 1\n    dim_ = x.shape[1]\n    resources_ = faiss.StandardGpuResources()\n    idx_config_ = faiss.GpuIndexFlatConfig()\n    idx_config_.useFloat16 = False\n    idx_config_.device = 0\n    index = faiss.GpuIndexFlatL2(resources_, dim_, idx_config_)\n    index.add(x)\n    _, neighbors = index.search(queries, k)\n    if with_queries:\n        for i in range(len(neighbors)):\n            indices = np.nonzero(neighbors[i, :] != i)[0]\n            indices.sort()\n            if len(indices) > k - 1:\n                indices = indices[:-1]\n            neighbors[i, :-1] = neighbors[i, indices]\n        neighbors = neighbors[:, :-1]\n    n_neighbors = np.array([[product_labels[i] for i in ii] for ii in neighbors])\n    index.reset()\n    resources_.noTempMemory()\n    del index\n    del x\n    gc.collect()\n    return n_neighbors\n\n\ndef compute_embeddings(_model, dataloader, with_embedding_layer=True, with_norm=True):\n    print('Computing embeddings..')\n    start_time = time.time()\n    _model.eval()\n    assert not _model.training\n\n    embeddings_set = [[] for _ in range(3)]\n    all_embeddings = []\n    with torch.no_grad():\n        for b, batch in enumerate(dataloader):\n            if b % 100 == 0:\n                print(f'batch {b}/{len(dataloader)}')\n            for ind, elems in enumerate(batch):\n                if ind == 0:\n                    if not with_embedding_layer:\n                        elems, embeddings = _model(elems.to(device), with_embedding_layer)\n                        all_embeddings.extend(embeddings.data.cpu().numpy())\n                    else:\n                        elems = _model(elems.to(device), with_embedding_layer)\n                    if with_norm:\n                        elems = f.normalize(elems, p=2, dim=1)\n                    elems = elems.data.cpu().numpy()\n                for elem in elems:\n                    embeddings_set[ind].append(np.asarray(elem))\n        result = [np.stack(embeddings_set[i]) for i in range(len(embeddings_set))]  # 3 x (samples_num x 2048)\n    print(f'computing embeddings time: {round((time.time() - start_time) / 60, 3)}, min')\n    if not with_embedding_layer:\n        return result, all_embeddings\n    return result\n\n\ndef cluster_embeddings(_model, dl, previous_centroids=None, is_finetune=False):\n    if is_finetune:\n        prod_labels = np.array(dl.dataset.labels)\n        emb_indexes = np.array(dl.dataset.ids)\n        cluster_labels_ = np.zeros(len(prod_labels), dtype=int)\n        centroids, embeddings, final_embeddings = None, None, None\n    else:\n        (embeddings, prod_labels, emb_indexes), final_embeddings = compute_embeddings(_model, dl, with_embedding_layer=False,\n                                                                                      with_norm=True)  # True\n        print(f'emb shapes while clustering: {embeddings.shape}')\n        sorted_lists = sort_(emb_indexes, [emb_indexes, embeddings, prod_labels])\n        emb_indexes, embeddings, prod_labels = sorted_lists[0], sorted_lists[1], sorted_lists[2]\n        start_time = time.time()\n        cluster_labels_, centroids = kmeans(\n            embeddings,\n            num_clusters=nb_clusters,\n            previous_centroids=previous_centroids)\n\n        print(f'clustering time: {round((time.time() - start_time) / 60, 3)} min')\n    return cluster_labels_, prod_labels, emb_indexes, centroids, embeddings, final_embeddings\n\n\ndef get_train_dl(idxs=None):\n    if cur_dataset is 'OnlineProducts':\n        ds_train_ = OnlineProductsDataset(ds_path=sop_dataset_path, mode='train', transforms_mode='train')\n    elif cur_dataset is 'InShopClothes':\n        ds_train_ = InShopDataset(ds_path=inshop_dataset_path, mode='train', transforms_mode='train')\n    elif cur_dataset is 'CUB200':\n        ds_train_ = CUBDataset(ds_path=cub_dataset_path, mode='train', transforms_mode='train')\n    elif cur_dataset is 'CARS196':\n        ds_train_ = CARSDataset(ds_path=cars_dataset_path, mode='train', transforms_mode='train')\n    ds_train_.get_within_indexes(idxs)\n    dl_train_ = torch.utils.data.DataLoader(ds_train_, batch_sampler=BatchSampler(ds_train_))\n    return dl_train_\n\ndef get_optimal_nb_clusters_by_loss(embeddings, prod_labels, previous_centroids, writer, epoch, criterion, nb_clusters):\n    start_time = time.time()\n    losses = []\n    distances_ap = []\n    distances_an = []\n    non_zeros_pos = []\n    non_zeros_neg = []\n    nb_clusters_list = []\n    total_nrof_triplets = []\n    total_neg_distances = []\n    print('start validate nb_clusters...')\n    for nb_clusters_ in range(8, 40):\n        cluster_labels_, centroids = kmeans(\n            embeddings,\n            num_clusters=nb_clusters_,\n            previous_centroids=previous_centroids)\n        print(nb_clusters_)\n        nb_clusters_list.append(nb_clusters_)\n        # print(f'distance: {distance}')\n        # writer.add_scalar('distance_%d'%nb_clusters_, distance, epoch)\n        current_loss = []\n        current_ap = []\n        current_an = []\n        current_non_zero_part_pos = []\n        current_non_zero_part_neg = []\n        current_nrof_triplets = []\n        current_nrof_not_null_loss = []\n        current_neg_dist = []\n        for cluster_id in np.unique(cluster_labels_):\n            indices = np.arange(len(embeddings),dtype=np.int)[cluster_labels_ == cluster_id]\n            loss, d_ap, d_an, d_pn, non_zero_part_pos, non_zero_part_neg, nrof_triplets, total_neg_dist, nrof_not_null_loss = criterion(torch.tensor(embeddings[indices]).cuda(),\n                                            torch.tensor(prod_labels[indices]).cuda(), epoch)\n            if np.isnan(loss.item()):\n                continue\n            current_loss.append(loss.item())\n            current_ap.append(d_ap.mean().item())\n            current_an.append(d_an.mean().item())\n            current_non_zero_part_pos.append(non_zero_part_pos.item())\n            current_non_zero_part_neg.append(non_zero_part_neg.item())\n            current_nrof_triplets.append(nrof_triplets)\n            current_neg_dist.append(total_neg_dist.item())\n            current_nrof_not_null_loss.append(nrof_not_null_loss)\n            print(current_loss[-1], current_ap[-1], current_an[-1])\n        losses.append(np.mean(current_loss))\n        distances_ap.append(np.mean(current_ap))\n        distances_an.append(np.mean(current_an))\n        non_zeros_pos.append(np.mean(current_non_zero_part_pos))\n        non_zeros_neg.append(np.mean(current_non_zero_part_neg))\n        total_nrof_triplets.append(np.mean(nrof_triplets))\n        total_neg_distances.append(np.mean(current_neg_dist))\n        writer.add_scalar('loss/epoch_%d' % epoch, np.mean(current_loss), nb_clusters_)\n        writer.add_scalar('d_ap/epoch_%d' % epoch, np.mean(current_ap), nb_clusters_)\n        writer.add_scalar('d_an/epoch_%d' % epoch, np.mean(current_an), nb_clusters_)\n        writer.add_scalar('non_zero_part_pos/epoch_%d' % epoch, np.mean(current_non_zero_part_pos), nb_clusters_)\n        writer.add_scalar('non_zero_part_neg/epoch_%d' % epoch, np.mean(current_non_zero_part_neg), nb_clusters_)\n        writer.add_scalar('nrof_triplets/epoch_%d' % epoch, np.mean(nrof_triplets), nb_clusters_)\n        writer.add_scalar('current_neg_dist/epoch_%d' % epoch, np.mean(current_neg_dist), nb_clusters_)\n        writer.add_scalar('current_nrof_not_null_loss/epoch_%d' % epoch, np.mean(current_nrof_not_null_loss), nb_clusters_)\n    plt.cla()\n    plt.plot(nb_clusters_list, losses, label='losses', color='r')\n    plt.plot(nb_clusters_list, non_zeros_pos, label='non_zeros_pos', color='b')\n    plt.plot(nb_clusters_list, non_zeros_neg, label='non_zeros_neg', color='g')\n    # plt.plot(nb_clusters_list, total_neg_distances, label='total_neg_distances', color='y')\n    plt.legend()\n    plt.savefig(tensorboard_dir+'different_nb_clusters_%d.png'%epoch)\n    plt.cla()\n    plt.plot(nb_clusters_list, total_nrof_triplets, label='total_nrof_triplets', color='b')\n    plt.savefig(tensorboard_dir+'total_nrof_triplets_different_nb_clusters_%d.png'%epoch)\n    # plt.show()\n    plt.cla()\n    print(f'clustering time: {round((time.time() - start_time) / 60, 3)} min')\n    best_index = int(np.argmax(losses))\n    nb_clusters_ = nb_clusters_list[best_index]\n    gc.collect()\n    print('optimal nb_clusters: %d, loss: %.3f, d_ap: %.3f, d_an: %.3f'% (nb_clusters_, losses[best_index],\n                                                                          distances_ap[best_index],\n                                                                          distances_an[best_index]))\n\ndef sort_(indexes_, lists_to_sort):\n    sorted_indexes = np.sort(indexes_)\n    sorted_lists = [list_[sorted_indexes] for list_ in lists_to_sort]\n    return sorted_lists\n\n\ndef round_neurons_num(neurons_percents):\n    def err(p, rounded_p):\n        d = np.sqrt(1.0 if p < 1.0 else p)\n        return abs(rounded_p - p) ** 2 / d\n\n    if not np.isclose(sum(neurons_percents), embedding_dim):\n        print(f'neurons_percents: {neurons_percents}')\n        raise ValueError\n    n = len(neurons_percents)\n    rounded = [int(x) for x in neurons_percents]\n    dif = embedding_dim - sum(rounded)\n    errs = [(err(neurons_percents[i], rounded[i] + 1) - err(neurons_percents[i], rounded[i]), i) for i in range(n)]\n    r = sorted(errs)\n    for i in range(dif):\n        rounded[r[i][1]] += 1\n    return rounded\n\n\ndef init_learners_masks(cluster_labels_, product_labels_, embeddings_):\n    print(f'initializing learners masks...')\n    st = time.time()\n    neurons_nmis = np.zeros((embedding_dim, nb_clusters))\n    for cluster_id in range(nb_clusters):\n        ids = np.where(cluster_labels_ == np.repeat(cluster_id, len(cluster_labels_)))[0]\n        cl_labels = cluster_labels_[ids]\n        prod_labels = product_labels_[ids]\n        prod_labels_count = len(np.unique(prod_labels))\n        embeds = embeddings_[ids]\n        assert len(embeds) == len(prod_labels)\n        neurons_outs = np.split(embeds, embedding_dim, axis=1)\n\n        for i, nrn_outs in enumerate(neurons_outs):\n            assert len(nrn_outs) == len(prod_labels)\n            nrn_cluster_lbls, _ = kmeans(nrn_outs, num_clusters=prod_labels_count, nrns_clustering=True)\n            assert len(nrn_cluster_lbls) == len(prod_labels)\n            nmi = nmi_(prod_labels, nrn_cluster_lbls)\n            neurons_nmis[i][cluster_id] = nmi\n    print(f'masks calculating time: {round((time.time() - st)/60, 3)} min')\n    return neurons_nmis\n\n\ndef calculate_neurons_nmi(cluster_labels_, product_labels_, embeddings_, epoch):\n    print(f'assigning neurons with nmi + lsa...')\n    st = time.time()\n    result = {}\n    embeddings_ = np.asarray(embeddings_)\n    for cluster_id in range(nb_clusters):\n        result[cluster_id] = {}\n        ids = np.where(cluster_labels_ == np.repeat(cluster_id, len(cluster_labels_)))[0]\n        # cl_labels = cluster_labels_[ids]\n        prod_labels = product_labels_[ids]\n        prod_labels_count = len(np.unique(prod_labels))\n        embeds = embeddings_[ids]\n        assert len(embeds) == len(prod_labels)\n\n        nrn_cluster_lbls, _ = kmeans(embeds, num_clusters=prod_labels_count, nrns_clustering=True)\n        assert len(nrn_cluster_lbls) == len(prod_labels)\n        nmi = nmi_(prod_labels, nrn_cluster_lbls)\n        result[cluster_id]['all_embedding'] = nmi\n        result[cluster_id]['exclude'] = {}\n        for i in range(embedding_dim):\n            # assert len(nrn_outs) == len(prod_labels)\n            nrn_outs = embeds[:, np.arange(embedding_dim)!= i]\n            assert len(nrn_outs[0]) == (embedding_dim-1)\n            nrn_cluster_lbls, _ = kmeans(nrn_outs, num_clusters=prod_labels_count, nrns_clustering=True)\n            assert len(nrn_cluster_lbls) == len(prod_labels)\n            nmi = nmi_(prod_labels, nrn_cluster_lbls)\n            result[cluster_id]['exclude'][i] = nmi\n        gc.collect()\n    with open(tensorboard_dir+'neurons_nmis_%d.json'%epoch, 'w') as file:\n        json.dump(result, file)\n    del result\n\n\ndef crazy_stuff(_model_,\n                dataloader_train,\n                writer,\n                epoch,\n                criterion,\n                is_reclustering_epoch=False,\n                previous_indexes=None,\n                previous_cluster_labels=None,\n                previous_centroids=None,\n                is_fine_tune=False,\n                is_fine_tune1=False):\n    \n    \n    if is_reclustering_epoch:\n        cluster_labels_, product_labels_, indexes_, centroids, embeddings, final_embeddings = cluster_embeddings(_model_,\n                                                                                               dataloader_train,\n                                                                                               previous_centroids=previous_centroids,\n                                                                                               is_finetune=is_fine_tune1)\n    else:\n        cluster_labels_, product_labels_, indexes_, centroids, embeddings, final_embeddings = cluster_embeddings(_model_,\n                                                                                               dataloader_train,\n                                                                                               is_finetune=is_fine_tune1)\n#     with torch.no_grad():\n#         get_optimal_nb_clusters_by_loss(np.asarray(final_embeddings), product_labels_, previous_centroids, writer, epoch, criterion, nb_clusters)\n    sorted_list = sort_(indexes_, [indexes_, product_labels_, cluster_labels_, embeddings])\n    indexes_, product_labels_, cluster_labels_, embeddings = sorted_list[0], sorted_list[1], sorted_list[2], \\\n                                                             sorted_list[3]\n\n    nrof_reassign_images = 0\n    for c in range(nb_clusters):\n        for t in np.unique(product_labels_[cluster_labels_ == c]):\n            condition = product_labels_[cluster_labels_ == c] == t\n            if (condition).sum().item() == 1:\n                if np.sum(product_labels_ == t).sum().item() == 1:\n                    cluster_labels_[(product_labels_ == t) & (cluster_labels_ == c)] = -1\n                    continue\n                nrof_reassign_images+=1\n                embedding = embeddings[cluster_labels_ == c, :][condition, :]\n                dist_to_clusters = np.linalg.norm(embedding-centroids, axis=1)\n                product_clusters = np.unique(cluster_labels_[product_labels_ == t])\n                dist_to_clusters[c] = 10e7\n                dist_to_clusters[np.logical_not(np.isin(np.arange(nb_clusters), product_clusters))] = 10e7\n                assert np.any(dist_to_clusters<10e6)\n                new_c = np.argmin(dist_to_clusters)\n                cluster_labels_[(product_labels_ == t) & (cluster_labels_ == c)] = new_c\n\n    log_metrics('clusters/nrof_unused_images', np.sum(cluster_labels_ == -1), epoch)        \n\n    # if not (is_fine_tune or is_fine_tune1) and epoch%10 == 0:\n    #     calculate_neurons_nmi(cluster_labels_, product_labels_, final_embeddings, epoch)\n    \n    # get NMI and loss of clusters\n    current_loss = [] \n    t1 = time.time()\n    for c in np.unique(cluster_labels_):\n        indices = np.arange(len(final_embeddings),dtype=np.int)[cluster_labels_ == c]\n        emb = torch.tensor([final_embeddings[ind] for ind in indices]).cuda()\n        prod_labels = torch.tensor([product_labels_[ind] for ind in indices]).cuda()\n        loss = criterion(emb, prod_labels, epoch, for_cluster=True)\n        if np.isnan(loss.item()):\n            continue\n        current_loss.append(loss.item())\n        cl_lbls, _ = kmeans(emb.cpu().numpy(), num_clusters=len(np.unique(prod_labels.cpu().numpy())))\n        cl_nmi = nmi_(cl_lbls, prod_labels.cpu().numpy()) * 100\n        log_metrics(f'clusters/cluster_{c}_nmi', cl_nmi, epoch)\n        log_metrics(f'clusters/cluster_{c}_loss', np.mean(current_loss), epoch)\n        print(f'cluster: {c}, nmi: {cl_nmi}, loss: {loss}')\n    print(f'getting loss and nmi for clusters time: {(time.time() - t1)/60} min')\n    \n    # get num_of_imgs_in_dif_clusters\n    nb_imgs_in_other_clusters = []\n    for i, pr_lbl in enumerate(np.unique(product_labels_)):\n        clusters_ids = cluster_labels_[product_labels_ == pr_lbl]\n        cnt = Counter(clusters_ids)\n        most_common_cluster, nb_imgs = cnt.most_common(1)[0]\n        count_in_other_clusters = np.sum(list(cnt.values())) - nb_imgs\n        nb_imgs_in_other_clusters.append(count_in_other_clusters)\n    part_imgs_from_other_clusters = np.sum(nb_imgs_in_other_clusters) / len(product_labels_)\n    print(f'part_imgs_from_other_clusters: {part_imgs_from_other_clusters}')\n    log_metrics(f'clusters/part_imgs_from_other_clusters1', part_imgs_from_other_clusters, epoch)\n    \n    dls = [[] for _ in range(nb_clusters)]\n    for cluster_id in range(nb_clusters):\n        idxs = indexes_[cluster_labels_ == cluster_id]\n        dls[cluster_id] = get_train_dl(idxs=idxs)\n    return dls, cluster_labels_, product_labels_, indexes_, centroids\n\n\ndef make_training_step(batch, learner_id, model, criterion, optimizer, iter_, epoch, cross_entropy=None, is_fine_tune=False):\n    images, product_labels = batch[0].cuda(), batch[1].cuda()\n    optimizer.zero_grad()\n    \n    if cross_entropy is not None:\n        embeddings, logits = model(images, with_softmax=True)\n        cls_loss = cross_entropy(logits, product_labels)*0.02\n        log_metrics('loss/cls_loss', cls_loss.item(), iter_)\n    else:\n        embeddings = model(images)\n        cls_loss = 0\n\n    if not is_fine_tune:\n        current_mask = model.masks(torch.tensor(learner_id).cuda())\n        current_mask = torch.nn.functional.relu(current_mask)\n        l1_regularization = lambda1 * torch.norm(current_mask, p=1)/embeddings.size(0)\n        l1_regularization = l1_regularization.cuda()\n        log_metrics('masks/nrof_positive_mask_%d'%learner_id, torch.nonzero(current_mask).size(0), iter_)\n        log_metrics('masks/mean_mask_%d'%learner_id, current_mask.mean().item(), iter_)\n        l2_regularization = lambda2 * embeddings.norm(2)/np.sqrt(embeddings.size(0))\n        embeddings = embeddings * current_mask\n    else:\n        l1_regularization = 0\n        l2_regularization = 0\n\n    l2_regularization = l2_regularization + lambda2 * embeddings.norm(2)/np.sqrt(embeddings.size(0))\n    l2_regularization = l2_regularization.cuda()\n        # embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    loss, d_ap, d_an, d_pn, non_zero_part_pos, non_zero_part_neg, nrof_triplets, total_neg_dist, nrof_not_null_loss, beta, unused_neg_part = \\\n        criterion(embeddings, product_labels, iter_, learner_id=learner_id)\n    loss = loss + l1_regularization + l2_regularization+cls_loss\n    if not is_fine_tune:\n        log_metrics('loss/L1_masks_reg', l1_regularization.item(), iter_)\n        log_metrics('loss/L1_masks_reg_learner_id_%d'%learner_id, l1_regularization.item(), iter_)\n    log_metrics('loss/L2_emb_reg', l2_regularization.item(), iter_)\n    log_metrics('loss/L2_emb_reg_learner_id_%d'%learner_id, l2_regularization.item(), iter_)\n    log_metrics('loss/total_loss', loss.item(), iter_)\n    log_metrics('distances/d_ap', d_ap.mean().item(), iter_)\n    log_metrics('distances/d_ap_learner_id_%d'%learner_id, d_ap.mean().item(), iter_)\n    log_metrics('distances/d_an', d_an.mean().item(), iter_)\n    log_metrics('distances/d_an_learner_id_%d'%learner_id, d_an.mean().item(), iter_)\n    if not torch.isnan(loss).any():\n        loss.backward()\n#         with amp.scale_loss(loss, optimizer) as scaled_loss:\n#             scaled_loss.backward()\n        optimizer.step()\n\n        with open(f'masks_epoch_{epoch}.json', 'w') as out_file:\n            masks_ = model.masks.weight.cpu().detach().numpy().tolist()\n            json.dump({'masks': masks_}, out_file)\n        log_artifacts(f'/kaggle/working/masks_epoch_{epoch}.json', f'masks_epoch_{epoch}.json')\n        \n        return loss.item(), d_ap, d_an, d_pn, non_zero_part_pos.item(), non_zero_part_neg.item(), nrof_triplets, total_neg_dist, nrof_not_null_loss, beta, unused_neg_part\n    else:\n        print(f'Loss is nan')\n        return None, d_ap, d_an, d_pn, non_zero_part_pos.item(), non_zero_part_neg.item(), nrof_triplets, total_neg_dist, nrof_not_null_loss, beta, unused_neg_part\n\n\ndef get_recall_at_k(product_labels, neighbours_ids, cur_k):\n    sum_ = 0\n    for query_lbl, nearest_ids in zip(product_labels, neighbours_ids):\n        if query_lbl in nearest_ids[:cur_k]:\n            sum_ += 1\n    recall_at_k = sum_ / len(product_labels)\n    return recall_at_k\n\n\ndef normalized(a, axis=-1, order=2):\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2==0] = 1\n    return a / np.expand_dims(l2, axis)\n\n\ndef eval_func_isc(query_embeddings, gallery_embeddings, query_product_labels, gallery_product_labels, k_list, nb_classes, writer, epoch, with_norm, use_mask):\n    if with_norm:\n        query_embeddings = normalized(np.copy(query_embeddings))\n        gallery_embeddings = normalized(np.copy(gallery_embeddings))\n    neighbours = get_nearest_neighbors(gallery_embeddings, gallery_product_labels, max(k_list),\n                                       queries=query_embeddings)\n    product_labels_all = np.concatenate([query_product_labels, gallery_product_labels])\n    embeddings_all = np.concatenate([query_embeddings, gallery_embeddings])\n    clusters_labels, _ = kmeans(embeddings_all, num_clusters=nb_classes)\n    nmi = nmi_(clusters_labels, product_labels_all) * 100\n    print(f'nmi: {nmi}')\n    out = []\n    gc.collect()\n    torch.cuda.empty_cache()\n    for k in k_list:\n        recall_at_k = get_recall_at_k(query_product_labels, neighbours, k) * 100\n        recall_metrics_name = f'Recall_at_{k}_w_norm' if with_norm else f'Recall_at_{k}_not_norm'\n        print(f'{recall_metrics_name}: {recall_at_k}')\n        if writer is not None:\n            writer.add_scalar(recall_metrics_name, recall_at_k, epoch)\n        log_metrics(f'eval/{recall_metrics_name}', recall_at_k, epoch)\n        out.append(recall_at_k)\n    out.append(nmi)\n    nmi_metrics_name = 'NMI_w_norm' if with_norm else 'NMI_not_norm'\n    if writer is not None:\n        writer.add_scalar(nmi_metrics_name, nmi, epoch)\n    log_metrics(f'eval/{nmi_metrics_name}', nmi, epoch)\n    str_ = ' '.join([str(elem) for elem in out])\n    with open(os.path.join(metrics_dir, f'metrics'), 'a') as f:\n        f.write(f'epoch_{epoch} norm {with_norm} ' + str_ + '\\n')\n    return out\n\n\ndef evaluate_isc(writer, epoch, is_fine_tune=False, with_norm=True, mode=None):  # for InShopClothes dataset\n    query_embeddings, query_product_labels, _ = compute_embeddings(model_, dl_query, with_norm=False)\n    gallery_embeddings, gallery_product_labels, _ = compute_embeddings(model_, dl_gallery, with_norm=False)    \n    \n    print(f'emb shape while evaluating: {query_embeddings.shape}, {gallery_embeddings.shape}')\n    nb_classes = dl_query.dataset.get_nb_classes()\n    assert nb_classes == len(set(query_product_labels))\n    original_query_embeddings = np.copy(query_embeddings)\n    original_gallery_embeddings = np.copy(gallery_embeddings)\n    k_list = [1, 10, 20, 30, 50]\n    for use_mask in ['not']:# + list(np.arange(8)):\n        for with_norm in [False, True]:\n            if use_mask == 'not':\n                query_embeddings = original_query_embeddings\n                gallery_embeddings = original_gallery_embeddings\n#             else:\n#                 query_embeddings = np.copy(original_query_embeddings) * torch.nn.functional.relu(model_.masks.weight[use_mask]).detach().cpu().numpy()\n#                 gallery_embeddings = np.copy(original_gallery_embeddings) * torch.nn.functional.relu(model_.masks.weight[use_mask]).detach().cpu().numpy()\n                                \n            out = eval_func_isc(query_embeddings, gallery_embeddings, query_product_labels, gallery_product_labels, k_list, nb_classes, writer, epoch, with_norm, use_mask)\n        gc.collect()\n    return out\n\n\ndef eval_func(embeddings, product_labels, k_list, nb_classes, writer, epoch, with_norm, use_mask):\n    if with_norm:\n        embeddings = normalized(np.copy(embeddings))\n    neighbours = get_nearest_neighbors(embeddings, product_labels, max(k_list))\n    clusters_labels, _ = kmeans(embeddings, num_clusters = nb_classes)\n    nmi = nmi_(clusters_labels, product_labels) * 100\n    print(f'nmi: {nmi}')\n    out = []\n    gc.collect()\n    torch.cuda.empty_cache()\n    for k in k_list:\n        recall_at_k = get_recall_at_k(product_labels, neighbours, k) * 100\n        recall_metrics_name = f'Recall_at_{k}_w_norm' if with_norm else f'Recall_at_{k}_not_norm'\n        print(f'{recall_metrics_name}: {recall_at_k}')\n        if writer is not None:\n            writer.add_scalar(recall_metrics_name, recall_at_k, epoch)\n        log_metrics(f'eval/{recall_metrics_name}', recall_at_k, epoch)\n        out.append(recall_at_k)\n    out.append(nmi)\n    nmi_metrics_name = 'NMI_w_norm' if with_norm else 'NMI_not_norm'\n    if writer is not None:\n        writer.add_scalar(nmi_metrics_name, nmi, epoch)\n    log_metrics(f'eval/{nmi_metrics_name}', nmi, epoch)\n    str_ = ' '.join([str(elem) for elem in out])\n    with open(os.path.join(metrics_dir, f'metrics'), 'a') as f:\n        f.write(f'epoch_{epoch} norm {with_norm} ' + str_ + '\\n')\n    return out\n\n\ndef evaluate(writer, epoch, dl_, is_fine_tune=False, with_norm=True, mode='test'):\n    # compute query images embeddings = retrieval images embeddings\n    print(f'Evaluating on {mode} data')\n    embeddings, product_labels, _ = compute_embeddings(model_, dl_, with_norm=False)\n    print(f'emb shape while evaluating: {embeddings.shape}')\n    nb_classes = dl_test.dataset.get_nb_classes()\n    original_embeddings = np.copy(embeddings)\n    if cur_dataset is 'CUB200' or cur_dataset is 'CARS196':\n        k_list = [1, 2, 4, 8]\n    elif cur_dataset is 'OnlineProducts': \n        k_list = [1, 10, 100, 1000]\n        \n    for use_mask in ['not']: # + list(np.arange(8)):\n        for with_norm in [False, True]:\n            if use_mask == 'not':\n                embeddings = original_embeddings\n#             else:\n#                 embeddings = np.copy(original_embeddings) * torch.nn.functional.relu(model_.masks.weight[use_mask]).detach().cpu().numpy()\n            out = eval_func(embeddings, product_labels, k_list, nb_classes, writer, epoch, with_norm, use_mask)\n        gc.collect()\n    return out\n\n\ndef get_gpu_memory():\n    gpu_resources = faiss.StandardGpuResources()\n    idx_config = faiss.GpuIndexFlatConfig()\n    idx_config.useFloat16 = False\n    idx_config.device = 0\n    index = faiss.GpuIndexFlatL2(gpu_resources, 2048, idx_config)\n    return index, gpu_resources\n\n\ndef save_all_data_to_tar(tarfile_name, cur_epoch):\n    try:\n        all_files = os.listdir('/kaggle/working/')\n        tar = tarfile.open(f\"{tarfile_name}.tar.gz\", \"w:gz\")\n        for item in all_files:\n            if not item.startswith('all') and f'_{cur_epoch}.' in item or f'_{cur_epoch - 1}.' in item or item.startswith('metrics'):\n                tar.add(item)\n        tar.close()\n    except:\n        print('error while writing to tarfile')\n\n        \nclass LabelSmoothLoss(torch.nn.Module):\n    def __init__(self, smoothing=0.0):\n        super(LabelSmoothLoss, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, input, target):\n        log_prob = torch.nn.functional.log_softmax(input, dim=-1)\n        weight = input.new_ones(input.size()) * \\\n                 self.smoothing / (input.size(-1) - 1.)\n        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n        loss = (-weight * log_prob).sum(dim=-1).mean()\n        return loss\n\ndef train(model, criterion, optimizer, all_betas):\n    if cur_dataset is 'OnlineProducts':\n        ds_train = OnlineProductsDataset(ds_path=sop_dataset_path, mode='train')\n    elif cur_dataset is 'InShopClothes':\n        ds_train = InShopDataset(ds_path=inshop_dataset_path, mode='train')\n    elif cur_dataset is 'CUB200':\n        ds_train = CUBDataset(ds_path=cub_dataset_path, mode='train')\n    elif cur_dataset is 'CARS196':\n        ds_train = CARSDataset(ds_path=cars_dataset_path, mode='train')\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size)\n\n    dl_train_list, cluster_labels, product_labels, indexes = None, None, None, None\n    is_fine_tune_epoch, is_fine_tune_epoch1 = False, False\n    start_epoch, global_step = 0, -1\n    \n#     if use_mean_beta_at_finetune:\n#         criterion[-1].beta.data = torch.tensor([torch.mean(torch.tensor([crit.beta.data for i, crit in enumerate(criterion[:-1])]))]).cuda()\n\n    if use_lr_scheduler:\n        num_steps = 200000\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=5e-7)\n        warmup_scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=100,\n                                                  after_scheduler=lr_scheduler)\n\n    if continue_training_from_epoch:\n        try:\n            checkpoint = torch.load(os.path.join(input_dir, f'checkpoint_{checkpoint_from_epoch}.pth'))\n            model.load_state_dict(checkpoint['model'])\n            start_epoch = checkpoint['epoch'] + 1\n            global_step = checkpoint['global_step'] + 1\n            optimizer.load_state_dict(checkpoint['opt'])\n            for state in optimizer.state.values():\n                for k, v in state.items():\n                    if isinstance(v, torch.Tensor):\n                        state[k] = v.to(device)\n#             amp.load_state_dict(checkpoint['amp_state_dict'])\n            for i, crit in enumerate(criterion):\n                crit.beta.data=checkpoint['beta'][i]\n            print(f'Loading model saved in epoch: {start_epoch - 1}')\n        except FileNotFoundError:\n            print('Checkpoint not found')\n\n        print('Trying to load saved clusters..')\n        try:\n            clusters_checkpoint = torch.load(os.path.join(input_dir, f'clusters_at_epoch_{clusters_from_epoch}.pth'))\n            cl_epoch = clusters_checkpoint['epoch'] + 1\n            dl_train_list = clusters_checkpoint['dl_train_list']\n            cluster_labels = clusters_checkpoint['cluster_labels']\n            indexes = clusters_checkpoint['indexes']\n            centroids = clusters_checkpoint['centroids']\n            print(f'Loading clusters saved in epoch: {cl_epoch - 1}')\n        except FileNotFoundError:\n            print('Clusters checkpoint not found')\n    \n    # model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n    cross_entropy = LabelSmoothLoss(0.1) if use_cls_loss else None  \n    optimizer.zero_grad()\n    optimizer.step()\n    if compute_metrics_before_training:\n        model.eval()\n        assert not model.training\n        with torch.no_grad():\n            if cur_dataset is 'InShopClothes':\n                metrics = evaluate_isc(None, -1)\n            else:\n                metrics = evaluate(None, -1, dl_=dl_test)\n        model.train()\n        assert model.training\n\n    for e in range(start_epoch, start_epoch + epochs):\n        if e >= ft_epoch:\n            is_fine_tune_epoch = True\n        if e >= fine_tune_epoch:\n            is_fine_tune_epoch1 = True\n        print(f'Epoch: {e}')\n        epoch_start_time = time.time()\n#         writer = SummaryWriter(log_dir= tensorboard_dir + f'/epoch_{e}')\n        writer = None\n\n        if e % recluster_epoch_freq == 0:\n            model.eval()\n            assert not model.training\n            print('Clustering..')\n            with torch.no_grad():\n                if e == 0:\n                    dl_train_list, cluster_labels, product_labels, indexes, centroids = \\\n                        crazy_stuff(model, dl_train, writer, e, criterion[-1], is_fine_tune=is_fine_tune_epoch,\n                                    is_fine_tune1=is_fine_tune_epoch1)\n                else:\n                    dl_train_list, cluster_labels, product_labels, indexes, centroids = \\\n                        crazy_stuff(model, dl_train, writer, e, criterion[-1], is_reclustering_epoch=True, previous_indexes=indexes,\n                                    previous_cluster_labels=cluster_labels, previous_centroids=centroids,\n                                    is_fine_tune=is_fine_tune_epoch, is_fine_tune1=is_fine_tune_epoch1)\n            for cluster_id in range(nb_clusters):\n                if writer is not None:\n                    writer.add_scalar('nb_images/cluster_%d'%cluster_id, np.sum(cluster_labels == cluster_id), e)\n                log_metrics('clusters/nb_images_cluster_%d'%cluster_id, np.sum(cluster_labels == cluster_id), e)\n            if save_clusters:\n                print('Saving clusters...')\n                clusters_checkpoint = {\n                    'epoch': e,\n                    'dl_train_list': dl_train_list,\n                    'cluster_labels': cluster_labels,\n                    'product_labels': product_labels,\n                    'indexes': indexes,\n                    'centroids': centroids\n                }\n                torch.save(clusters_checkpoint,\n                           (os.path.join(checkpoints_dir, f'clusters_at_epoch_{e}.pth')))\n\n        model.train()\n        assert model.training\n        print('Starting training..')\n        nb_batches = [len(dl) for dl in dl_train_list]\n        print('nb_bathes in each dl', nb_batches)\n        for nb_b in nb_batches:\n            log_metrics('clusters/nb_batches', nb_b, e)\n        if cur_dataset is 'CUB200' or cur_dataset is 'CARS196':\n            _nb_batches_ = min(nb_batches)\n        else:\n            _nb_batches_ = int(np.mean(nb_batches))  # max(nb_batches)\n        loss_list, d_ap_list, d_an_list, d_pn_list, beta_list, unused_neg_part_list = list(), list(), list(), list(), list(), list()\n        loss_by_learners, d_ap_by_learners, d_an_by_learners, d_pn_by_learners, beta_by_learners = {i:[] for i in range(nb_clusters)}, {i:[] for i in range(nb_clusters)}, {i:[] for i in range(nb_clusters)}, {i:[] for i in range(nb_clusters)}, {i:[] for i in range(nb_clusters)}\n        unused_neg_part_by_learners = {i:[] for i in range(nb_clusters)}\n        nb_iters = _nb_batches_ * nb_clusters\n        print(f'Iterations num: {nb_iters}')\n        log_metrics('clusters/Iterations num', nb_iters, e)\n        with open(os.path.join(metrics_dir, f'model.masks'), 'a') as f:\n            f.write(f'epoch_{e} ' + str(model.masks) + '\\n')\n        \n        gc.collect()\n        for j in range(_nb_batches_):\n            for i, dl in enumerate(dl_train_list):\n                if len(dl) != 0:\n                    dl = iter(dl)\n                    batch = next(dl)\n                    if batch is None:\n                        dl = iter(dl)\n                        batch = next(dl)\n                    learner_id = i\n                    loss_id = -1 if is_fine_tune_epoch else i\n                    \n                    with torch.autograd.set_detect_anomaly(True):\n                        loss_on_batch, d_ap, d_an, d_pn, non_zero_part_pos, non_zero_part_neg, \\\n                        nrof_triplets, total_neg_dist, nrof_not_null_loss, beta, unused_neg_part = make_training_step(batch, learner_id, model, criterion[loss_id], optimizer,\n                                                                       global_step, e, cross_entropy=cross_entropy,\n                                                                       is_fine_tune=is_fine_tune_epoch)\n                        if loss_on_batch != None:\n                            global_step += 1\n    #                         if writer is not None:\n                            log_metrics('loss/Loss_train', loss_on_batch, global_step)\n                            log_metrics(f'loss/_loss_learner_{learner_id}', loss_on_batch, global_step)\n                            log_metrics(f'distances/d_ap_learner_{learner_id}', d_ap.mean().item(), global_step)\n                            log_metrics(f'distances/d_an_learner_{learner_id}', d_an.mean().item(), global_step)\n                            log_metrics(f'distances/d_pn_learner_{learner_id}', d_pn.mean().item(), global_step)\n\n                            log_metrics('loss/non_zero_part_pos_loss', non_zero_part_pos, global_step)\n                            log_metrics(f'loss/non_zero_part_pos_loss_learner_{learner_id}', loss_on_batch, global_step)\n\n                            log_metrics('loss/non_zero_part_neg_loss', non_zero_part_neg, global_step)\n                            log_metrics(f'loss/non_zero_part_neg_loss_learner_{learner_id}', non_zero_part_neg, global_step)\n\n                            log_metrics('loss/nrof_not_null_loss', nrof_not_null_loss, global_step)\n                            log_metrics(f'loss/nrof_not_null_loss_learner_{learner_id}', nrof_not_null_loss, global_step)\n\n                            log_metrics(f'clusters/nrof_triplets_learner_{learner_id}', nrof_triplets, global_step)\n                            log_metrics(f'clusters/nrof_triplets_all', nrof_triplets, global_step)\n                            log_metrics(f'distances/total_neg_dist_learner_{learner_id}', total_neg_dist.item(), global_step)\n                            log_metrics(f'distances/total_neg_dist_all', total_neg_dist.item(), global_step)\n                            for i in range(len(criterion_)):\n                                log_metrics(f'loss/beta_learner_id_{i}', criterion_[i].beta[0].item(), global_step)\n\n                            loss_list.append(loss_on_batch)\n                            loss_by_learners[learner_id].append(loss_on_batch)\n                            d_ap_by_learners[learner_id].append(d_ap.data.cpu().numpy().tolist())\n                            d_an_by_learners[learner_id].append(d_an.data.cpu().numpy().tolist())\n                            d_pn_by_learners[learner_id].append(d_pn.data.cpu().numpy().tolist())\n                            unused_neg_part_by_learners[learner_id].append(unused_neg_part)\n                            beta_by_learners[learner_id].append(beta)\n                            \n                            d_ap_list.append(d_ap.data.cpu().numpy().tolist())\n                            d_an_list.append(d_an.data.cpu().numpy().tolist())\n                            d_pn_list.append(d_pn.data.cpu().numpy().tolist())\n                            beta_list.append(beta)\n                            unused_neg_part_list.append(unused_neg_part)\n\n                            if global_step % 50 == 0:\n                                if global_step != 0:\n                                    loss_mean = np.mean(loss_list[-50:])\n                                else:\n                                    loss_mean = loss_on_batch     \n                                print(f'global step: {global_step}, loss: {loss_mean}')\n            \n            if use_lr_scheduler:\n                lr_scheduler.step(global_step)\n                warmup_scheduler.step(global_step)\n\n                for param_group in optimizer.param_groups:\n                    if writer is not None:\n                        writer.add_scalar('lr', param_group['lr'], global_step)\n                    log_metrics('loss/lr', param_group['lr'], global_step)\n                    continue\n                \n        log_metrics('distances/d_ap_mean', np.mean(d_ap_list), e)\n        log_metrics('distances/d_an_mean', np.mean(d_an_list), e)\n        log_metrics('distances/d_pn_mean', np.mean(d_pn_list), e)\n        log_metrics('loss/mean_loss', np.mean(loss_list), e)\n        log_metrics('loss/mean_beta', np.mean(beta_list), e)\n        log_metrics('clusters/mean_unused_w_cutoff_neg_part', np.mean(unused_neg_part_list), e)\n        \n        for learner_id in range(nb_clusters):\n            log_metrics('loss/mean_loss_learner_id_%d'%learner_id, np.mean(loss_by_learners[learner_id]), e)\n            log_metrics('distances/mean_d_ap_learner_id_%d'%learner_id, np.mean(d_ap_by_learners[learner_id]), e)\n            log_metrics('distances/mean_d_an_learner_id_%d'%learner_id, np.mean(d_an_by_learners[learner_id]), e)\n            log_metrics('distances/mean_d_pn_learner_id_%d'%learner_id, np.mean(d_pn_by_learners[learner_id]), e)\n            log_metrics('loss/mean_beta_learner_id_%d'%learner_id, np.mean(beta_by_learners[learner_id]), e)\n            log_metrics('clusters/mean_unused_w_cutoff_neg_part_learner_id_%d'%learner_id, np.mean(unused_neg_part_by_learners[learner_id]), e)\n        \n        if to_make_density_plots and e % 2 == 0 and not is_fine_tune_epoch1:\n            print(f'making density plots..')\n            d_ap_list_ = np.concatenate(d_ap_list, 0)\n            d_an_list_ = np.concatenate(d_an_list, 0)\n            make_density_plots(e, d_ap_list_, d_an_list_, mode='train')\n\n        print(f'epoch training time: {round((time.time() - epoch_start_time) / 60, 3)} min')\n        eval_start_time = time.time()\n        model.eval()\n        assert not model.training\n        with torch.no_grad():\n            if cur_dataset is 'InShopClothes':\n                metrics = evaluate_isc(writer, e)\n            else:\n                metrics = evaluate(writer, e, dl_=dl_test)\n                if evalaute_on_train_data:\n                    metrics = evaluate(writer, e, dl_=dl_train, mode='train')\n                    \n        if use_mean_beta_at_finetune and e == ft_epoch - 1:\n            criterion[-1].beta.data = torch.tensor([torch.mean(torch.tensor([crit.beta.data for i, crit in enumerate(criterion[:-1])]))]).cuda()   \n#             print('beta at finetune start', criterion[-1].beta.data)\n        \n        if save_model:\n            print('Saving current model...')\n            state = {\n                'model': model.state_dict(),\n#                 'amp_state_dict': amp.state_dict(),\n                'epoch': e,\n                'global_step': global_step,\n                'opt': optimizer.state_dict(),\n                'beta': [criterion_[i].beta for i in range(len(criterion_))]\n            }\n            torch.save(state, (os.path.join(checkpoints_dir, f'checkpoint_{e}.pth')))\n            \n        files_to_save = os.listdir('/kaggle/working/')\n        tar = tarfile.open(f'mlruns_epoch_{e}.tar.gz', 'w:gz')\n        tar1 = tarfile.open(f'all_metrics_{e}.tar.gz', 'w:gz')\n        for item in files_to_save:\n            if item.startswith('mlruns') and not item.startswith('mlruns_epoch'):\n                tar.add(item)\n            if item in dirs_for_logs:\n                tar1.add(item)\n        tar.close()\n        tar1.close()\n        \n#         save_all_data_to_tar(f'all_epoch_{e}', e)\n        \n        if os.path.exists(os.path.join(checkpoints_dir, f'checkpoint_{e-5}.pth')):\n            os.remove(os.path.join(checkpoints_dir, f'checkpoint_{e-5}.pth'))\n            \n        if os.path.exists(os.path.join(checkpoints_dir, f'clusters_at_epoch_{e-10}.pth')):\n            os.remove(os.path.join(checkpoints_dir, f'clusters_at_epoch_{e-10}.pth'))\n            \n        if os.path.exists(os.path.join(checkpoints_dir, f'mlruns_epoch_{e-5}.tar.gz')):\n            os.remove(os.path.join(checkpoints_dir, f'mlruns_epoch_{e-5}.tar.gz'))\n            \n#         if os.path.exists(os.path.join(checkpoints_dir, f'all_epoch_{e-2}.tar.gz')):\n#             os.remove(os.path.join(checkpoints_dir, f'all_epoch_{e-2}.tar.gz'))\n            \n        print(f'validation time: {round((time.time() - eval_start_time) / 60, 3)} min')\n        print(f'Epoch total time: {round((time.time() - epoch_start_time) / 60, 3)} min')\n\ndef test(model, epoch):\n    print(f'testing model..')\n    if load_saved_model_for_test:\n        try:\n            checkpoint = torch.load(os.path.join(input_dir, f'checkpoint_{epoch}.pth'))\n            model.load_state_dict(checkpoint['model'])\n            cur_epoch = checkpoint['epoch']\n            print(f'Loading model saved in epoch: {cur_epoch}')\n        except FileNotFoundError:\n            print('Checkpoint not found')\n    model.eval()\n    assert not model.training\n    with torch.no_grad():\n        if cur_dataset is 'InShopClothes':\n            metrics = evaluate_isc(None, -1)\n        else:\n            metrics = evaluate(None, -1)\n\n\nif __name__ == '__main__':\n    print(f'selected dataset: {cur_dataset}')\n    torch.cuda.set_device(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n    # memory = get_gpu_memory()\n\n    if cur_dataset is 'OnlineProducts':\n        train_classes, test_classes = sop_train_classes, sop_test_classes\n        ds_test = OnlineProductsDataset(ds_path=sop_dataset_path, mode='test')\n        dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size)\n        nb_train_data = sop_nb_train_data\n\n    elif cur_dataset is 'InShopClothes':\n        train_classes, test_classes = inshop_train_classes, inshop_test_classes\n        ds_query = InShopDataset(ds_path=inshop_dataset_path, mode='query')\n        ds_gallery = InShopDataset(ds_path=inshop_dataset_path, mode='gallery')\n        dl_query = torch.utils.data.DataLoader(ds_query, batch_size=batch_size)\n        dl_gallery = torch.utils.data.DataLoader(ds_gallery, batch_size=batch_size)\n        nb_train_data = inshop_nb_train_data\n\n    elif cur_dataset is 'CUB200':\n        train_classes, test_classes = cub_train_classes, cub_test_classes\n        ds_test = CUBDataset(ds_path=cub_dataset_path, mode='test')\n        dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size)\n        nb_train_data = cub_nb_train_data\n\n    elif cur_dataset is 'CARS196':\n        train_classes, test_classes = cars_train_classes, cars_test_classes\n        ds_test = CARSDataset(ds_path=cars_dataset_path, mode='test')\n        dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size)\n        nb_train_data = cars_nb_train_data\n\n    model_ = get_model().cuda()\n    criterion_ = get_criterion()\n    all_betas = [criterion_[i].beta for i in range(len(criterion_))]\n    optimizer_ = get_optimizer(model_, all_betas)\n#     model_, optimizer_ = amp.initialize(model_, optimizer_, opt_level='O1')\n\n    #     test(model_, 186)\n\n    total_training_start_time = time.time()\n    train(model_, criterion_, optimizer_, all_betas)\n    print(f'Training time: {round((time.time() - total_training_start_time) / 60, 3)} min')\n\n    # test(model_, 0)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"9ee630ec-6f33-45ba-a989-03f0aaa6c2af","_cell_guid":"23746eee-dd14-4761-9ab4-7dfbaed11b51","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}